{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IF2A682ZP3qN"
   },
   "source": [
    "## Generative Adversarial Networks on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will create and train a GAN to generate images of digits that mimic those in the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5EpzTB6P3qO"
   },
   "source": [
    "### Evaluation metric: Inception Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4Gb-AhYP3qO"
   },
   "source": [
    "Rather than just eye-balling whether GAN samples look good or not, researchers have come up with mulitple objective metrics for determining the quality and the diversity of GAN outputs. We will use one of the metrics called the *Inception Score*.\n",
    "\n",
    "Calculating the Inception Score involves running a pretrained neural network. This is where the name is from: the authors who proposed this metric used a pretrained [Inception Network](https://arxiv.org/pdf/1409.4842.pdf) from Tensorflow in their [paper](https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf). Since we will be using the MNIST dataset in this assignment, we provide a simpler neural work pretrained on MNIST as the scoring model.\n",
    "\n",
    "The idea behind the Inception Score is simple: a good GAN should generate *meaningful* and *diverse* samples. For MNIST, a specific sample is \"meaningful\" if it looks like any of the 10 digits. When we take a good digit classifier and run it on this sample, it should assign high probability to one of the 10 classes and low probability to the others. In information theory terms, this means the predicted label distribution $p(y|x)$ for any specific sample $x$ should have high entropy. On the other hand, if the generated samples are diverse, they should be able to cover all 10 classes when we generate a large enough set of samples. This means that the \"average\" label distribution $p(y) = \\int p(y|x=G(z)) \\mathrm{d}z$ should have low entropy. The Inception Score is define by $\\exp (\\mathbb{E}_x \\mathrm{KL}(p(y|x) || p(y)))$, where $\\mathrm{KL}(P||Q)$ is the K-L divergence, which is often used to measure how probability distribution $P$ is different from distribution $Q$. Intuitively, if the generated samples are good, $p(y|x)$ should be different from $p(y)$, since one should have high entropy while the other should have low entropy.\n",
    "\n",
    "Don't be too worried if you don't fully get how the score is defined and calculated. Just remember that in this assignment, we want our GAN to have a high Inception Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "U51Pv7gnP3qO"
   },
   "outputs": [],
   "source": [
    "# Pretrained model used to evaluation the inception score.\n",
    "class ScoringModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "def inception_score_mnist(\n",
    "    imgs,\n",
    "    model_path='weights/mnist.ckpt',\n",
    "    batch_size=32,\n",
    "    num_splits=10,\n",
    "):\n",
    "    \"\"\"Computes the inception score of `imgs`.\n",
    "    \n",
    "    Args:\n",
    "    - imgs: Array of size (number of data points, 1, 28, 28)\n",
    "    - batch_size: Batch size for feeding data into the pretrained MNIST model.\n",
    "    - num_splits: Number of splits. We split the samples into multiple subsets\n",
    "        and calculate the scores on each of them. Their mean is used as the\n",
    "        final score.\n",
    "    \"\"\"\n",
    "    # Verify that input arguments have the correct formats.\n",
    "    assert type(imgs) == np.ndarray\n",
    "    assert imgs.shape[1:] == (1, 28, 28)\n",
    "    assert batch_size > 0\n",
    "    assert len(imgs) > batch_size\n",
    "    \n",
    "    # Choose device to be used.\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Preprocess input.\n",
    "    imgs = copy.copy(imgs)\n",
    "    imgs = (imgs - 0.1307) / 0.3081\n",
    "    \n",
    "    # Set up dataloader.\n",
    "    dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
    "\n",
    "    # Load pretrained scoring model.\n",
    "    model = ScoringModel()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get predictions.\n",
    "    preds = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "        preds.append(probs)\n",
    "    preds = np.concatenate(preds)\n",
    "\n",
    "    # Compute the mean KL divergence.\n",
    "    split_scores = []\n",
    "\n",
    "    for i in range(num_splits):\n",
    "        n = len(imgs) // num_splits\n",
    "        split = preds[i*n:(i+1)*n, :]\n",
    "        py = np.mean(split, axis=0)\n",
    "        scores = []\n",
    "        for i in range(split.shape[0]):\n",
    "            pyx = split[i, :]\n",
    "            scores.append(entropy(pyx, py))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "    \n",
    "    return np.mean(split_scores), np.std(split_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsyZJXAOP3qO"
   },
   "source": [
    "Now, let's try to calculate the Inception score on the actual MNIST dataset.\n",
    "\n",
    "Make sure that the provided file `mnist.ckpt` is under `./weights`. Alternatively, you can specify its path via the `model_path` argument of `inception_score_mnist()`. If using Google Colab, click `View > Table of Contents > Files` and then upload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JlXCEGNBP3qO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (500, 1, 28, 28)\n",
      "Inception Score: mean=8.958, std=0.438\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.ToTensor()\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data', train=False, download=True, transform=transform),\n",
    "    batch_size=500, shuffle=True)\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "x = x.cpu().data.numpy()\n",
    "x = x.reshape((-1,1,28,28))\n",
    "print('Shape of data:',x.shape)\n",
    "mean, std = inception_score_mnist(x)\n",
    "print(f'Inception Score: mean={mean:.3f}, std={std:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[[[0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         ...,\n         [0.        , 0.        , 0.        , ..., 0.26666668,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ]]],\n\n\n       [[[0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         ...,\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ]]],\n\n\n       [[[0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         ...,\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ]]],\n\n\n       ...,\n\n\n       [[[0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         ...,\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ]]],\n\n\n       [[[0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         ...,\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ]]],\n\n\n       [[[0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         ...,\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ],\n         [0.        , 0.        , 0.        , ..., 0.        ,\n          0.        , 0.        ]]]], dtype=float32)"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPpHfMZ_P3qO"
   },
   "source": [
    "The score for the real MNIST dataset should be above 8.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjoMze3uP3qO"
   },
   "source": [
    "### Generating MNIST images (100 points)\n",
    "\n",
    "As you did with the Gaussian distribution example in the weekly notebook, define and train a GAN to generate images that mimic those in the MNIST dataset.\n",
    "\n",
    "#### Deliverables\n",
    "\n",
    "- After training your model, generate at least 1500 samples using the trained generator, and evaluate your model by calculating the Inception score on the generated samples.\n",
    "- Pick a few generated samples and visualize them.\n",
    "- Plot the training losses for the discriminator and the generator.\n",
    "\n",
    "Given the limited computational resources, you will want to achieve an Inception score of 1.5 or greater for full credits. A score of 1.5 won't yield great images. For nice looking images, you'll need an Inception score of around 6.0, but it is not needed for full credits.\n",
    "\n",
    "#### Model Submission\n",
    "\n",
    "For more complicated architectures, if your model takes a long time to train, you will need to save the model and write a code snippet that loads it such that the code runs with no errors and we can grade it easily. In this case, set `epochs = 0` and include the saved model in your submission (or a Google drive share link if its too large).\n",
    "\n",
    "#### Tips\n",
    "\n",
    "- It will be easier to get better results with a convolutional GAN. You may find this [tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html) on [DCGAN](https://arxiv.org/pdf/1511.06434.pdf) helpful. The generators of DCGANs make use of transposed convolutions (`nn.ConvTranspose2d` in PyTorch) to map features to larger sizes. This [article](https://d2l.ai/chapter_computer-vision/transposed-conv.html) does a good job illustrating how they work.\n",
    "- Feel free to try different architectures, layers, optimizers, training schemes and other hyperparameters. Particularly, if training with one type of optimizer is slow or unstable, give other types of optimizers a try.\n",
    "\n",
    "There are plenty of online resources about GAN that you can reference for inspiration. But do not plagiarize. Please write your own custom networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the training data\n",
    "train = pd.read_csv('data/mnist_csv_format/mnist_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_data = train.drop(labels = ['label'], axis = 1)\n",
    "train_data = train_data.values.reshape(-1, 28, 28)\n",
    "train_data = train_data/255.0\n",
    "\n",
    "#To create some space\n",
    "del train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#Just checking if the data type is as expected\n",
    "print(isinstance(train_data, np.ndarray))\n",
    "\n",
    "#Checking the sanity of the shape of the training data\n",
    "print(train_data.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x1c6aa318b80>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_data[7], cmap = 'gray')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#Converting to torch tensor\n",
    "train_data = torch.Tensor(train_data)\n",
    "\n",
    "#Checking to confirm dtype as tensor\n",
    "print(isinstance(train_data, torch.Tensor))\n",
    "\n",
    "random_seed = 1\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle = True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def random_noise_generator(batch_size, dim):\n",
    "    return torch.rand(batch_size, dim)*2 - 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x1c6aaffd6a0>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Just checking the noise generator and plotting one of its outputs\n",
    "a = random_noise_generator(64, 100)\n",
    "b = a[2]\n",
    "b = b.reshape(10, 10)\n",
    "b = b.numpy()\n",
    "plt.imshow(b, cmap = 'gray')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        #self.conv0_bn = nn.BatchNorm2d(32)\n",
    "        self.conv0_drop = nn.Dropout2d(0.25)\n",
    "        self.conv1 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        #self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        self.conv1_drop = nn.Dropout2d(0.25)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\n",
    "        #self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        self.conv2_drop = nn.Dropout2d(0.25)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size = 3, stride = 2, padding = 1)\n",
    "        #self.conv3_bn = nn.BatchNorm2d(256)\n",
    "        self.conv3_drop = nn.Dropout2d(0.25)\n",
    "        self.fc = nn.Linear(12544, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "        x = F.leaky_relu(self.conv0(x), 0.2)\n",
    "        #x = self.conv0_bn(x)\n",
    "        x = self.conv0_drop(x)\n",
    "        x = F.leaky_relu(self.conv1(x), 0.2)\n",
    "        #x = self.conv1_bn(x)\n",
    "        x = self.conv1_drop(x)\n",
    "        x = F.leaky_relu(self.conv2(x), 0.2)\n",
    "        #x = self.conv2_bn(x)\n",
    "        x = self.conv2_drop(x)\n",
    "        x = F.leaky_relu(self.conv3(x), 0.2)\n",
    "        #x = self.conv3_bn(x)\n",
    "        x = self.conv3_drop(x)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "\n",
    "        return num_features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(100, 256*7*7)\n",
    "        self.trans_conv1 = nn.ConvTranspose2d(256, 128, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "        #self.trans_conv1_bn = nn.BatchNorm2d(128)\n",
    "        self.trans_conv2 = nn.ConvTranspose2d(128, 64, kernel_size = 3, stride = 1, padding = 1)\n",
    "        #self.trans_conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.trans_conv3 = nn.ConvTranspose2d(64, 32, kernel_size = 3, stride = 1, padding = 1)\n",
    "        #self.trans_conv3_bn = nn.BatchNorm2d(32)\n",
    "        self.trans_conv4 = nn.ConvTranspose2d(32, 1, kernel_size = 3, stride = 2, padding = 1, output_padding = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 256, 7, 7)\n",
    "        x = F.relu(self.trans_conv1(x))\n",
    "        #x = self.trans_conv1_bn(x)\n",
    "        x = F.relu(self.trans_conv2(x))\n",
    "        #x = self.trans_conv2_bn(x)\n",
    "        x = F.relu(self.trans_conv3(x))\n",
    "        #x = self.trans_conv3_bn(x)\n",
    "        x = self.trans_conv4(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv0_drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (conv3_drop): Dropout2d(p=0.25, inplace=False)\n",
      "  (fc): Linear(in_features=12544, out_features=1, bias=True)\n",
      ")\n",
      "Generator(\n",
      "  (fc): Linear(in_features=100, out_features=12544, bias=True)\n",
      "  (trans_conv1): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "  (trans_conv2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (trans_conv3): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (trans_conv4): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator()\n",
    "G = Generator()\n",
    "\n",
    "print(D)\n",
    "print(G)\n",
    "\n",
    "#Passing to the GPU\n",
    "D = D.to(device)\n",
    "G = G.to(device)\n",
    "\n",
    "D = D.float()\n",
    "G = G.float()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "Loss = nn.BCEWithLogitsLoss()\n",
    "def discriminator_real_loss(real_out):\n",
    "    real_label = torch.ones(real_out.size()[0], 1).to(device)\n",
    "    real_loss = Loss(real_out.squeeze(), real_label.squeeze())\n",
    "    return real_loss\n",
    "\n",
    "def discriminator_fake_loss(fake_out):\n",
    "    fake_label = torch.zeros(fake_out.size()[0], 1).to(device)\n",
    "    fake_loss = Loss(fake_out.squeeze(), fake_label.squeeze())\n",
    "    return fake_loss\n",
    "\n",
    "def discriminator_loss(real_out, fake_out):\n",
    "    real_loss = discriminator_real_loss(real_out)\n",
    "    fake_loss = discriminator_fake_loss(fake_out)\n",
    "    total_loss = (real_loss + fake_loss)\n",
    "    return total_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def generator_loss(gen_disc_out):\n",
    "    label = torch.ones(gen_disc_out.size()[0], 1).to(device)\n",
    "    gen_loss = Loss(gen_disc_out.squeeze(), label.squeeze())\n",
    "    return gen_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "disc_opt = optim.Adam(D.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "gen_opt = optim.Adam(G.parameters(), lr = 0.0002, betas = (0.5, 0.999))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def train(D, G, disc_opt, gen_opt, train_dl, batch_size = 32, epochs = 25, gen_input_size = 100):\n",
    "\n",
    "    disc_losses = []\n",
    "    gen_losses = []\n",
    "\n",
    "    #Having a fixed sample to monitor the progress of the generator\n",
    "    sample_size = 16\n",
    "    fixed_samples = random_noise_generator(sample_size, gen_input_size)\n",
    "    fixed_samples = fixed_samples.to(device)\n",
    "\n",
    "    #Going into training mode\n",
    "    D.train()\n",
    "    G.train()\n",
    "\n",
    "    for epoch in range(epochs + 1):\n",
    "\n",
    "        disc_loss_total = 0\n",
    "        gen_loss_total = 0\n",
    "        gen_out = 0\n",
    "\n",
    "        for train_x in train_dl:\n",
    "\n",
    "            #Discriminator training\n",
    "            disc_opt.zero_grad()\n",
    "\n",
    "            train_x = train_x*2 - 1          #Converting the real images to have values between -1 and 1\n",
    "            train_x = train_x.to(device)     #Passing to GPU\n",
    "            real_out = D(train_x.float())\n",
    "\n",
    "            disc_gen_in = random_noise_generator(batch_size, gen_input_size)\n",
    "            disc_gen_in = disc_gen_in.to(device)   #Passing to GPU\n",
    "\n",
    "            disc_gen_out = G(disc_gen_in.float()).detach()  #Detaching to avoid training the generator\n",
    "            fake_out = D(disc_gen_out.float())\n",
    "\n",
    "            disc_loss = discriminator_loss(real_out, fake_out)  #Loss calculation\n",
    "            disc_loss_total += disc_loss\n",
    "            disc_loss.backward()\n",
    "            disc_opt.step()\n",
    "\n",
    "            #Generator training\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "\n",
    "            gen_out = G(disc_gen_in.float())     #Feeding noise into the generator\n",
    "            gen_disc_out = D(gen_out.float())       #Passing into the discrminator\n",
    "\n",
    "            gen_loss = generator_loss(gen_disc_out)  #Generator loss calculation\n",
    "            gen_loss_total += gen_loss\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "        disc_losses.append(disc_loss_total)\n",
    "        gen_losses.append(gen_loss_total)\n",
    "\n",
    "        #Plotting samples every 5 epochs\n",
    "        if epoch%5 == 0:\n",
    "            G.eval()                    #Going into eval mode to get sample images\n",
    "            samples = G(fixed_samples.float())\n",
    "            G.train()                   #Going back into train mode\n",
    "\n",
    "            fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
    "            for ax, img in zip(axes.flatten(), samples):\n",
    "               img = img.cpu().detach()\n",
    "               ax.xaxis.set_visible(False)\n",
    "               ax.yaxis.set_visible(False)\n",
    "               im = ax.imshow(img.reshape((28,28)), cmap='Greys_r')\n",
    "\n",
    "\n",
    "        #Printing losses every epoch\n",
    "        print(\"Epoch \", epoch, \": Discriminator Loss = \", disc_loss_total/len(train_dl), \", Generator Loss = \", gen_loss_total/len(train_dl))\n",
    "\n",
    "    return disc_losses, gen_losses"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "disc_losses, gen_losses = train(D, G, disc_opt, gen_opt, train_dl, batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " GAN_lab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
